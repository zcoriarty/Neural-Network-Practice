{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 24,
            "source": [
                "\n",
                "# matplotlib for plotting\n",
                "import matplotlib\n",
                "matplotlib.rcParams['figure.figsize'] = (10.0, 10.0)\n",
                "from matplotlib import pyplot as plt\n",
                "\n",
                "# numpy for vector and matrix manipulations\n",
                "import numpy as np\n",
                "\n",
                "# for data manipulation\n",
                "from sklearn.metrics import accuracy_score\n",
                "from sklearn.utils import shuffle\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "source": [
                "def activation(z, type):\n",
                "    \"\"\"\n",
                "    Activation function that takes input z and returns activated z \n",
                "    Sigmoid: output lies between zero and 1\n",
                "    Tanh: output lies between -1 and 1\n",
                "    \"\"\"\n",
                "    if type == 'sigmoid':\n",
                "        return 1 / (1 + np.exp(-z))\n",
                "    if type == 'tanh':\n",
                "        return np.tanh(z)\n",
                "\n",
                "def activation_derivative(z, type):\n",
                "    '''\n",
                "    Finds the derivative of the activation function, given z\n",
                "    From the definition f'(x) = f(x) * (1 - f(x))\n",
                "    '''\n",
                "    if type == 'sigmoid':\n",
                "        sig = activation(z, 'sigmoid')\n",
                "        return sig * (1 - sig)\n",
                "    if type == 'tanh':\n",
                "        tanh = activation(z, 'tanh')\n",
                "        return tanh * (1 - tanh)\n",
                "\n",
                "def mse(y_true, y_pred):\n",
                "    \"\"\"\n",
                "    Calculates the Mean Squared Error between the predicted value and the ground truth\n",
                "    - for calculating loss\n",
                "    \"\"\"\n",
                "    n = y_pred.shape[1]\n",
                "    cost = (1./(2*n)) * np.sum((y_true - y_pred) ** 2)\n",
                "    return cost\n",
                "\n",
                "def sensitivity(y_true, y_pred):\n",
                "    \"\"\"\n",
                "    Calculates the difference between predicted and ground truth \n",
                "    \"\"\"\n",
                "    cost_prime = y_pred - y_true\n",
                "    return cost_prime\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "source": [
                "class NeuralNetwork(object):\n",
                "\n",
                "    def __init__(self, size, seed=86):\n",
                "        '''\n",
                "        Instantiate the weights and biases of the network\n",
                "        that will be updated during training\n",
                "        '''\n",
                "        self.seed = seed\n",
                "        np.random.seed(self.seed)\n",
                "        self.size = size\n",
                "        self.weights = [np.random.randn(self.size[i], self.size[i-1]) * np.sqrt(1 / self.size[i-1]) for i in range(1, len(self.size))]\n",
                "        self.biases = [np.random.rand(n, 1) for n in self.size[1:]]\n",
                "\n",
                "    def forward_propagation(self, input):\n",
                "        '''\n",
                "        Computes the forward propagation matrix calculations for a given input\n",
                "        '''\n",
                "        a = input\n",
                "        pre_activations = []\n",
                "        activations = [a]\n",
                "        for w, b in zip(self.weights, self.biases):\n",
                "            z = np.dot(w, a) + b\n",
                "            a  = activation(z, 'sigmoid')\n",
                "            pre_activations.append(z)\n",
                "            activations.append(a)\n",
                "        return a, pre_activations, activations\n",
                "\n",
                "    def compute_deltas(self, pre_activations, y_true, y_pred):\n",
                "        \"\"\"\n",
                "        Calculates the delta between layers\n",
                "        \"\"\"\n",
                "        delta_L = sensitivity(y_true, y_pred) * activation_derivative(pre_activations[-1], 'sigmoid')\n",
                "        deltas = [0] * (len(self.size) - 1)\n",
                "        deltas[-1] = delta_L\n",
                "        for l in range(len(deltas) - 2, -1, -1):\n",
                "            delta = np.dot(self.weights[l + 1].transpose(), deltas[l + 1]) * activation_derivative(pre_activations[l], 'sigmoid') \n",
                "            deltas[l] = delta\n",
                "        return deltas\n",
                "\n",
                "    def backpropagate(self, deltas, pre_activations, activations):\n",
                "        \"\"\"\n",
                "        Computes the derivative of the loss w.r.t weight and bias\n",
                "        \"\"\"\n",
                "        dW = []\n",
                "        db = []\n",
                "        deltas = [0] + deltas\n",
                "        for l in range(1, len(self.size)):\n",
                "            dW_l = np.dot(deltas[l], activations[l-1].transpose()) \n",
                "            db_l = deltas[l]\n",
                "            dW.append(dW_l)\n",
                "            db.append(np.expand_dims(db_l.mean(axis=1), 1))\n",
                "        return dW, db\n",
                "\n",
                "    def train(self, X, y, batch_size, epochs, learning_rate, validation_split=0.2, print_every=10, plot_every=None):\n",
                "        \"\"\"\n",
                "        Trains the network and outputs accuracy and loss\n",
                "        \"\"\"\n",
                "\n",
                "        x_train, x_test, y_train, y_test = train_test_split(X.T, y.T, test_size=validation_split, )\n",
                "        x_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T \n",
                "            \n",
                "        epoch_iterator = range(epochs)\n",
                "\n",
                "        for e in epoch_iterator:\n",
                "\n",
                "            # calculate the number of batches\n",
                "            if x_train.shape[1] % batch_size == 0:\n",
                "                n_batches = int(x_train.shape[1] / batch_size)\n",
                "            else:\n",
                "                n_batches = int(x_train.shape[1] / batch_size ) - 1\n",
                "\n",
                "            # randomize and transpose data\n",
                "            x_train, y_train = shuffle(x_train.T, y_train.T)\n",
                "            x_train, y_train = x_train.T, y_train.T\n",
                "\n",
                "            # list of batches based on the inputted batch size\n",
                "            batches_x = [x_train[:, batch_size*i:batch_size*(i+1)] for i in range(0, n_batches)]\n",
                "            batches_y = [y_train[:, batch_size*i:batch_size*(i+1)] for i in range(0, n_batches)]\n",
                "\n",
                "            # for tracking accuracy/loss\n",
                "            train_losses, train_accuracies, test_losses, test_accuracies = [], [], [], []\n",
                "\n",
                "            # hold averages\n",
                "            average_train_losses, average_train_accuracies, average_test_losses, average_test_accuracies = [], [], [], []\n",
                "\n",
                "            dw_per_epoch = [np.zeros(w.shape) for w in self.weights]\n",
                "            db_per_epoch = [np.zeros(b.shape) for b in self.biases] \n",
                "            \n",
                "            for batch_x, batch_y in zip(batches_x, batches_y):\n",
                "                batch_y_pred, pre_activations, activations = self.forward_propagation(batch_x)\n",
                "                deltas = self.compute_deltas(pre_activations, batch_y, batch_y_pred)\n",
                "                dW, db = self.backpropagate(deltas, pre_activations, activations)\n",
                "                for i, (dw_i, db_i) in enumerate(zip(dW, db)):\n",
                "                    dw_per_epoch[i] += dw_i / batch_size\n",
                "                    db_per_epoch[i] += db_i / batch_size\n",
                "\n",
                "                batch_y_train_pred = self.predict(batch_x)\n",
                "\n",
                "                train_loss = mse(batch_y, batch_y_train_pred)\n",
                "                train_losses.append(train_loss)\n",
                "                train_accuracy = accuracy_score(batch_y.T, batch_y_train_pred.T)\n",
                "                train_accuracies.append(train_accuracy)\n",
                "\n",
                "                batch_y_test_pred = self.predict(x_test)\n",
                "\n",
                "                test_loss = mse(y_test, batch_y_test_pred)\n",
                "                test_losses.append(test_loss)\n",
                "                test_accuracy = accuracy_score(y_test.T, batch_y_test_pred.T)\n",
                "                test_accuracies.append(test_accuracy)\n",
                "\n",
                "\n",
                "            # weight update\n",
                "            for i, (dw_epoch, db_epoch) in enumerate(zip(dw_per_epoch, db_per_epoch)):\n",
                "                self.weights[i] = self.weights[i] - learning_rate * dw_epoch\n",
                "                self.biases[i] = self.biases[i] - learning_rate * db_epoch\n",
                "\n",
                "            average_train_losses.append(np.mean(train_losses))\n",
                "            average_train_accuracies.append(np.mean(train_accuracies))\n",
                "            \n",
                "            average_test_losses.append(np.mean(test_losses))\n",
                "            average_test_accuracies.append(np.mean(test_accuracies))\n",
                "\n",
                "        history = {'epochs': epochs,\n",
                "                   'train_loss': average_train_losses, \n",
                "                   'train_acc': average_train_accuracies,\n",
                "                   'test_loss': average_test_losses,\n",
                "                   'test_acc': average_test_accuracies\n",
                "                   }\n",
                "        return history\n",
                "\n",
                "    def predict(self, a):\n",
                "        '''\n",
                "        Make a prediction on the trained network for a given state 'a'\n",
                "        '''\n",
                "        for w, b in zip(self.weights, self.biases):\n",
                "            z = np.dot(w, a) + b\n",
                "            a = activation(z, 'sigmoid')\n",
                "        predictions = (a > 0.5).astype(int)\n",
                "        return predictions"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "source": [
                "'''\n",
                "Used for testing against sklearn datasets\n",
                "'''\n",
                "from sklearn import datasets\n",
                "\n",
                "data = datasets.make_moons(n_samples=1000, noise=0.1)\n",
                "X = data[0].T\n",
                "y = np.expand_dims(data[1], 1).T\n",
                "\n",
                "# takes a list with num layers = length of list and num nodes for layer i = list[i-1]\n",
                "neural_net = NeuralNetwork([2, 4, 2, 1], seed=2)\n",
                "history = neural_net.train(X=X, y=y, batch_size=32, epochs=500, learning_rate=0.4, print_every=200, validation_split=0.2, plot_every=15)\n",
                "print('Test Accuracy:', history['test_acc'][-1],\n",
                "\t'\\nTrain Accuracy:', history['train_acc'][-1],\n",
                "\t'\\nTest Loss:', history['test_loss'][-1],\n",
                "\t'\\nTrain Loss:', history['train_loss'][-1])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Test Accuracy: 0.98 \n",
                        "Train Accuracy: 0.985 \n",
                        "Test Loss: 0.01 \n",
                        "Train Loss: 0.0075\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.2",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.2 64-bit"
        },
        "interpreter": {
            "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}